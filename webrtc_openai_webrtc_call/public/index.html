<!doctype html>
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI-mediated WebRTC chat (mic-first, accurate logs)</title>

<style>
  body{font-family:sans-serif;text-align:center;margin-top:2em}
  input,button{font-size:1.05em} button{margin-left:.3em}
  #clickToUnmute{position:fixed;bottom:1.5rem;left:50%;
                 transform:translateX(-50%);background:#000d;color:#fff;
                 padding:.6em 1.2em;border-radius:9999px;cursor:pointer;
                 z-index:1000}
  #clickToUnmute[hidden]{display:none}
  pre{max-width:90%;margin:1em auto;background:#f3f3f3;
      padding:.7em;text-align:left}
</style>

<h2>WebRTC chat ‚Äì model‚Äôs voice only for the remote peer</h2>
<p>
  Room:
  <input id="roomInput" placeholder="choose-a-room">
  <button id="joinBtn" disabled>Join</button>
</p>

<audio id="remoteAudio" autoplay playsinline></audio>
<div id="clickToUnmute" hidden>üîä Tap to un-mute</div>
<pre id="log"></pre>

<script>
/* ---------- tiny logger ---------- */
const logEl=document.getElementById('log');
function log(...a){console.log('[DBG]',...a);
  logEl.textContent+=a.map(v=>typeof v==='string'?v:JSON.stringify(v)).join(' ')+'\n';}

/* ---------- UI ---------- */
const roomInput   =document.getElementById('roomInput');
const joinBtn     =document.getElementById('joinBtn');
const remoteAudio =document.getElementById('remoteAudio');
const unmuteBadge =document.getElementById('clickToUnmute');

roomInput.oninput=()=>joinBtn.disabled=roomInput.value.trim()==='';
joinBtn.onclick  =joinRoom;
unmuteBadge.onclick=()=>{remoteAudio.muted=false;remoteAudio.play();unmuteBadge.hidden=true;};

/* ---------- globals ---------- */
let pcOai,pcChat,ws,statsTimer,relayCtx;

/* helper: wait for ICE complete */
const waitIce=pc=>new Promise(r=>{
  pc.iceGatheringState==='complete'?r():
  pc.addEventListener('icegatheringstatechange',function h(){
    pc.iceGatheringState==='complete'&&(pc.removeEventListener('icegatheringstatechange',h),r());
  });
});

/* ------ OpenAI connection (unchanged) ------ */
async function connectToOpenAI(micTrack){ ‚Ä¶ /* same as previous answer */ }

/* ------ retryPlay & buildChatPC (unchanged) ------ */
function retryPlay(elem,attempt=0){ ‚Ä¶ }               // same as previous
function buildChatPC(firstTrack){ ‚Ä¶ }                  // same as previous
async function negotiateChat(){ ‚Ä¶ }                    // same as previous

/* ---------- entry point ---------- */
async function joinRoom(){
  joinBtn.disabled=true;
  log('joinRoom pressed');

  /* 1. ask for mic immediately */
  let localStream;
  try{
    localStream = await navigator.mediaDevices.getUserMedia({audio:true});
    log('mic permission granted');
  }catch(e){
    alert('Need microphone access'); joinBtn.disabled=false; return;
  }
  const micTrack = localStream.getAudioTracks()[0];

  /* 2. now create the relay AudioContext and resume it */
  relayCtx = new (window.AudioContext||window.webkitAudioContext)();
  if(relayCtx.state==='suspended'){
    await relayCtx.resume();
  }
  log('AudioContext state:', relayCtx.state);   // ‚Äúrunning‚Äù or ‚Äúinterrupted‚Äù

  /* 3. prime <audio> so autoplay is unlocked */
  remoteAudio.muted=true;
  remoteAudio.srcObject=new MediaStream();
  try{await remoteAudio.play();}catch{}

  /* 4. everything else in parallel */
  connectToOpenAI(micTrack).catch(console.error);      // no await ‚Üí parallel
  ws = new WebSocket(`${location.protocol==='https:'?'wss':'ws'}://${location.host}/ws`);
  ws.onopen =()=>{ws.send(JSON.stringify({join:roomInput.value.trim()})); negotiateChat();};
  ws.onmessage = async ev => {
    const msg = JSON.parse(typeof ev.data==='string'?ev.data:await ev.data.text());
    if(msg.offer){
      await pcChat.setRemoteDescription(msg.offer);
      await pcChat.setLocalDescription(await pcChat.createAnswer());
      ws.send(JSON.stringify({answer:pcChat.localDescription}));
    }else if(msg.answer){
      await pcChat.setRemoteDescription(msg.answer);
    }else if(msg.ice){try{await pcChat.addIceCandidate(msg.ice);}catch{}}
  };
}
</script>