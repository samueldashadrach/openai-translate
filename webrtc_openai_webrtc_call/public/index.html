<!doctype html>
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI-mediated WebRTC chat (audio-relay fixed)</title>

<style>
  body{font-family:sans-serif;text-align:center;margin-top:2em}
  input,button{font-size:1.05em} button{margin-left:.3em}
  #clickToUnmute{position:fixed;bottom:1.5rem;left:50%;transform:translateX(-50%);
                 background:#000d;color:#fff;padding:.6em 1.2em;border-radius:9999px;
                 cursor:pointer;z-index:1000}
  #clickToUnmute[hidden]{display:none}
  pre{max-width:90%;margin:1em auto;background:#f3f3f3;padding:.7em;text-align:left}
</style>

<h2>WebRTC chat â€“ modelâ€™s voice only for the remote peer</h2>
<p>
  Room:
  <input id="roomInput" placeholder="choose-a-room">
  <button id="joinBtn" disabled>Join</button>
</p>

<audio id="remoteAudio" autoplay playsinline></audio>
<div id="clickToUnmute" hidden>ðŸ”Š Tap to un-mute</div>
<pre id="log"></pre>

<script>
/* ------------ tiny logger ------------ */
const logEl=document.getElementById('log');
const log=(...a)=>{console.log('[DBG]',...a);
                   logEl.textContent+=a.map(v=>typeof v==='string'?v:JSON.stringify(v)).join(' ')+'\n';};

/* ------------ globals ------------ */
let pcOai,pcChat,ws,statsTimer;
let relayCtx;                        // single AudioContext kept running

/* ------------ UI ------------ */
const roomInput   =document.getElementById('roomInput');
const joinBtn     =document.getElementById('joinBtn');
const remoteAudio =document.getElementById('remoteAudio');
const unmuteBadge =document.getElementById('clickToUnmute');

roomInput.oninput=()=>joinBtn.disabled=roomInput.value.trim()==='';
joinBtn.onclick  =joinRoom;
unmuteBadge.onclick=()=>{remoteAudio.muted=false;remoteAudio.play();unmuteBadge.hidden=true;};

/* helper: wait for ICE complete */
const waitIce=(pc)=>new Promise(r=>pc.iceGatheringState==='complete'?r():
  pc.addEventListener('icegatheringstatechange',function h(){if(pc.iceGatheringState==='complete'){pc.removeEventListener('icegatheringstatechange',h);r();}}));

/* ---------- OpenAI connection ---------- */
async function connectToOpenAI(micTrack){
  log('Fetching token â€¦');
  const {client_secret}=await(await fetch('/session')).json();
  const token=client_secret.value;

  pcOai=new RTCPeerConnection({iceServers:[{urls:'stun:stun.l.google.com:19302'}]});
  pcOai.addTrack(micTrack);

  /* relay model â†’ chat */
  pcOai.ontrack=({streams})=>{
    const modelStream=streams[0];
    log('<< modelStream',modelStream.id);

    /* convert *remote* track â†’ *local* with the already-running context */
    const src = relayCtx.createMediaStreamSource(modelStream);
    const dest= relayCtx.createMediaStreamDestination();
    src.connect(dest);
    const localTrack=dest.stream.getAudioTracks()[0];

    if(!pcChat){buildChatPC(localTrack); if(ws?.readyState===1) negotiateChat();}
    else{
      const sender=pcChat.getSenders().find(s=>s.track?.kind==='audio');
      sender?sender.replaceTrack(localTrack):pcChat.addTrack(localTrack);
    }
  };

  await pcOai.setLocalDescription(await pcOai.createOffer());
  await waitIce(pcOai);

  const answer=await fetch(
    'https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2025-06-03',
    {method:'POST',headers:{Authorization:`Bearer ${token}`,'Content-Type':'application/sdp'},
     body:pcOai.localDescription.sdp}).then(r=>r.text());
  await pcOai.setRemoteDescription({type:'answer',sdp:answer});
  log('OpenAI SDP done â€“ waiting for media');
}

/* ---------- browser â†” browser ---------- */
function buildChatPC(firstTrack){
  pcChat=new RTCPeerConnection({iceServers:[{urls:'stun:stun.l.google.com:19302'}]});
  pcChat.addTrack(firstTrack);
  pcChat.onicecandidate=({candidate})=>candidate&&ws?.send(JSON.stringify({ice:candidate}));
  pcChat.ontrack=({track,streams})=>{
    log('>> ontrack',track.id);
    remoteAudio.srcObject=streams[0];
    remoteAudio.muted=false;
    const p=remoteAudio.play();
    if(p?.catch) p.catch(e=>{log('autoplay blocked');unmuteBadge.hidden=false;});
  };

  clearInterval(statsTimer);
  statsTimer=setInterval(async()=>{
    const s=await pcChat.getStats();let inB=0,outB=0;
    s.forEach(r=>{if(r.type==='inbound-rtp'&&r.kind==='audio')inB+=r.bytesReceived;
                  if(r.type==='outbound-rtp'&&r.kind==='audio')outB+=r.bytesSent;});
    log('RTP bytes â–²',outB,' â–¼',inB);
  },3000);
}

async function negotiateChat(){
  await pcChat.setLocalDescription(await pcChat.createOffer());
  ws.send(JSON.stringify({offer:pcChat.localDescription}));
}

/* ---------- entry point ---------- */
async function joinRoom(){
  joinBtn.disabled=true;
  log('joinRoom pressed');

  /* prime audio element + create & resume AudioContext while click counts */
  remoteAudio.muted=true;
  try{remoteAudio.play();}catch{}
  relayCtx=new (window.AudioContext||window.webkitAudioContext)();
  await relayCtx.resume();           // now context is running âœ…
  log('AudioContext resumed');

  /* mic */
  let localStream;
  try{localStream=await navigator.mediaDevices.getUserMedia({audio:true});}
  catch{alert('Need microphone access');joinBtn.disabled=false;return;}
  const micTrack=localStream.getAudioTracks()[0];

  await connectToOpenAI(micTrack);

  /* signalling WS */
  ws=new WebSocket(`${location.protocol==='https:'?'wss':'ws'}://${location.host}/ws`);
  ws.onopen=()=>{ws.send(JSON.stringify({join:roomInput.value.trim()})); negotiateChat();};
  ws.onmessage=async ev=>{
    const msg=JSON.parse(typeof ev.data==='string'?ev.data:await ev.data.text());
    if(msg.offer){
      await pcChat.setRemoteDescription(msg.offer);
      await pcChat.setLocalDescription(await pcChat.createAnswer());
      ws.send(JSON.stringify({answer:pcChat.localDescription}));
    }else if(msg.answer){
      await pcChat.setRemoteDescription(msg.answer);
    }else if(msg.ice){try{await pcChat.addIceCandidate(msg.ice);}catch{}}
  };
}
</script>