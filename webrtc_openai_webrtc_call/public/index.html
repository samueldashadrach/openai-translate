
<!doctype html>
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI-mediated WebRTC chat</title>

<style>
  body  { font-family:sans-serif; text-align:center; margin-top:2em; }
  input,button{ font-size:1.1em; }
  button{ margin-left:.3em; }
</style>

<h2>WebRTC chat - audio passes through GPT-4o first</h2>
<p>
  Room:
  <input id="roomInput" placeholder="choose-a-room">
  <button id="joinBtn" disabled>Join</button>
</p>

<audio id="remoteAudio" autoplay playsinline></audio>     <!-- hears OTHER user -->
<audio id="modelAudio"  autoplay playsinline style="display:none"></audio> <!-- AI -->

<script>
/* ---------- UI ---------- */
const roomInput   = document.getElementById('roomInput');
const joinBtn     = document.getElementById('joinBtn');
const remoteAudio = document.getElementById('remoteAudio');
const modelAudio  = document.getElementById('modelAudio');

roomInput.addEventListener('input', () =>
  joinBtn.disabled = roomInput.value.trim() === ''
);
joinBtn.addEventListener('click', joinRoom);

/* ---------- globals ---------- */
let pcOai, pcChat, ws;              // OpenAI-PC, chat-PC, signalling WS

/* ---------- OpenAI connection ---------- */
async function connectToOpenAI(micTrack) {
  /* 1. fetch 1-minute token from backend */
  const { client_secret } = await (await fetch('/session')).json();
  const EPHEMERAL = client_secret.value;

  /* 2. RTCPeerConnection to api.openai.com */
  pcOai = new RTCPeerConnection();
  pcOai.addTrack(micTrack);

  /* optional data-channel for transcripts / function-calls */
  const dc = pcOai.createDataChannel('oai-events');
  dc.onmessage = ev => {
    try { console.log('OAI event', JSON.parse(ev.data)); }
    catch { console.log('OAI raw', ev.data); }
  };

  /* 3. when model speaks back */
  pcOai.ontrack = ({ streams }) => {
    const modelStream = streams[0];
    modelAudio.srcObject = modelStream;           // user also hears the AI

    /* turn remote track into a *local* track we can send further */
    const ctx  = new AudioContext();
    const src  = ctx.createMediaStreamSource(modelStream);
    const dest = ctx.createMediaStreamDestination();
    src.connect(dest);
    const sendTrack = dest.stream.getAudioTracks()[0];

    if (!pcChat) {
      buildChatPC(sendTrack);     // make the chat connection now
      if (ws && ws.readyState === WebSocket.OPEN) negotiateChat();
    } else {
      // replace existing outbound audio
      const sender = pcChat.getSenders()
                           .find(s => s.track && s.track.kind === 'audio');
      if (sender) sender.replaceTrack(sendTrack);
      else pcChat.addTrack(sendTrack);
    }
  };

  /* 4. SDP offer/answer with OpenAI */
  const offer = await pcOai.createOffer();
  await pcOai.setLocalDescription(offer);

  const ansTxt = await fetch(
    'https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2025-06-03',
    {
      method : 'POST',
      headers: {
        Authorization: `Bearer ${EPHEMERAL}`,
        'Content-Type': 'application/sdp'
      },
      body: offer.sdp
    }
  ).then(r => r.text());

  await pcOai.setRemoteDescription({ type:'answer', sdp: ansTxt });
}

/* ---------- browser â†” browser ---------- */
function buildChatPC(firstTrack) {
  pcChat = new RTCPeerConnection({
    iceServers: [{ urls:'stun:stun.l.google.com:19302' }]
  });
  if (firstTrack) pcChat.addTrack(firstTrack);

  pcChat.onicecandidate = ({ candidate }) => {
    if (candidate && ws?.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({ ice:candidate }));
    }
  };
  pcChat.ontrack = ({ streams }) => remoteAudio.srcObject = streams[0];
}

async function negotiateChat() {
  if (!pcChat) return;
  if (location.hash !== '#callee') {           // we are the offerer
    const offer = await pcChat.createOffer();
    await pcChat.setLocalDescription(offer);
    ws.send(JSON.stringify({ offer }));
  }
}

/* ---------- entry point ---------- */
async function joinRoom() {
  joinBtn.disabled = true;

  /* mic */
  let localStream;
  try {
    localStream = await navigator.mediaDevices.getUserMedia({
      audio:{ echoCancellation:true, noiseSuppression:true }
    });
  } catch {
    alert('Need microphone access'); joinBtn.disabled=false; return;
  }
  const micTrack = localStream.getAudioTracks()[0];

  /* OpenAI */
  await connectToOpenAI(micTrack);

  /* signalling WS */
  const proto = location.protocol === 'https:' ? 'wss' : 'ws';
  ws = new WebSocket(`${proto}://${location.host}/ws`);
  ws.onopen = () => {
    ws.send(JSON.stringify({ join:roomInput.value.trim() }));
    negotiateChat();               // in case pcChat already exists
  };

  ws.onmessage = async ev => {
    let data = (typeof ev.data === 'string')
                 ? ev.data
                 : await ev.data.text();
    let msg; try { msg = JSON.parse(data); } catch { return; }

    if (msg.offer) {
      await pcChat.setRemoteDescription(msg.offer);
      const answer = await pcChat.createAnswer();
      await pcChat.setLocalDescription(answer);
      ws.send(JSON.stringify({ answer }));
      location.hash = '#callee';
    } else if (msg.answer) {
      await pcChat.setRemoteDescription(msg.answer);
    } else if (msg.ice) {
      try { await pcChat.addIceCandidate(msg.ice); } catch {}
    }
  };
}
</script>