<!doctype html>
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI-mediated WebRTC chat</title>

<style>
  body  { font-family:sans-serif; text-align:center; margin-top:2em; }
  input,button{ font-size:1.05em; }
  button{ margin-left:.3em; }

  /* â”€â”€â”€ small unobtrusive un-mute badge â”€â”€â”€ */
  #clickToUnmute{
    position:fixed;
    bottom:1.5rem; left:50%; transform:translateX(-50%);
    background:#000d; color:#fff;
    padding:.6em 1.2em;
    border-radius:9999px;
    cursor:pointer;
    z-index:1000;
  }
  #clickToUnmute[hidden]{ display:none; }

  pre{ text-align:left; max-width:90%; margin:1em auto;
       background:#f3f3f3; padding:.7em; }
</style>

<h2>WebRTC chat â€“ modelâ€™s voice only for the remote peer</h2>
<p>
  Room:
  <input id="roomInput" placeholder="choose-a-room">
  <button id="joinBtn" disabled>Join</button>
</p>

<audio id="remoteAudio" autoplay playsinline></audio>

<!-- unobtrusive badge â€“ appears only if autoplay was blocked -->
<div id="clickToUnmute" hidden>
  ðŸ”Š Tap to unmute
</div>

<pre id="log"></pre>

<script>
/* ---------- tiny logger ---------- */
const logEl = document.getElementById('log');
function log(...a){
  const txt = a.map(v=>typeof v==='string'?v:JSON.stringify(v)).join(' ');
  console.log('[DBG]', ...a);
  logEl.textContent += txt + '\n';
}

/* ---------- UI ---------- */
const roomInput   = document.getElementById('roomInput');
const joinBtn     = document.getElementById('joinBtn');
const remoteAudio = document.getElementById('remoteAudio');
const unmuteBadge = document.getElementById('clickToUnmute');

roomInput.addEventListener('input', () =>
  joinBtn.disabled = roomInput.value.trim() === ''
);
joinBtn.addEventListener('click', joinRoom);
unmuteBadge.addEventListener('click', ()=>{
  remoteAudio.muted = false;
  remoteAudio.play().catch(()=>{});
  unmuteBadge.hidden = true;
});

/* ---------- globals ---------- */
let pcOai, pcChat, ws;

/* helper: wait for ICE to finish */
const waitIceComplete = pc => new Promise(res=>{
  if(pc.iceGatheringState==='complete') return res();
  pc.addEventListener('icegatheringstatechange', function h(){
    if(pc.iceGatheringState==='complete'){ pc.removeEventListener('icegatheringstatechange',h); res(); }
  });
});

/* ---------- OpenAI connection ---------- */
async function connectToOpenAI(micTrack){
  const { client_secret } = await (await fetch('/session')).json();
  const EPHEMERAL = client_secret.value;

  pcOai = new RTCPeerConnection({ iceServers:[{urls:'stun:stun.l.google.com:19302'}] });
  pcOai.addTrack(micTrack);

  pcOai.ontrack = ({ streams })=>{
    log('<< model audio stream received');
    const modelStream = streams[0];

    /* turn remote â†’ local */
    const ctx  = new (AudioContext||webkitAudioContext)();
    const src  = ctx.createMediaStreamSource(modelStream);
    const dest = ctx.createMediaStreamDestination();
    src.connect(dest);
    const localTrack = dest.stream.getAudioTracks()[0];

    if(!pcChat){
      buildChatPC(localTrack);
      if(ws?.readyState===WebSocket.OPEN) negotiateChat();
    }else{
      const sender = pcChat.getSenders().find(s=>s.track?.kind==='audio');
      sender ? sender.replaceTrack(localTrack) : pcChat.addTrack(localTrack);
    }
  };

  await pcOai.setLocalDescription(await pcOai.createOffer());
  await waitIceComplete(pcOai);

  const answer = await fetch(
    'https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2025-06-03',
    { method:'POST',
      headers:{ Authorization:`Bearer ${EPHEMERAL}`, 'Content-Type':'application/sdp' },
      body:pcOai.localDescription.sdp }
  ).then(r=>r.text());
  await pcOai.setRemoteDescription({ type:'answer', sdp:answer });
}

/* ---------- browser â†” browser ---------- */
function buildChatPC(firstTrack){
  pcChat = new RTCPeerConnection({ iceServers:[{urls:'stun:stun.l.google.com:19302'}] });
  pcChat.addTrack(firstTrack);

  pcChat.onicecandidate = ({candidate})=>{
    if(candidate && ws?.readyState===WebSocket.OPEN)
      ws.send(JSON.stringify({ice:candidate}));
  };

  pcChat.ontrack = ({ streams })=>{
    log('>> peer ontrack');
    remoteAudio.srcObject = streams[0];

    remoteAudio.muted = false;
    remoteAudio.play()
      .then(()=>log('remote audio playing'))
      .catch(e =>{ log('autoplay blocked'); unmuteBadge.hidden=false; });
  };
}

async function negotiateChat(){
  const offer = await pcChat.createOffer();
  await pcChat.setLocalDescription(offer);
  ws.send(JSON.stringify({offer}));
}

/* ---------- entry point ---------- */
async function joinRoom(){
  joinBtn.disabled = true;

  /* prime audio element while click is still considered a user gesture */
  remoteAudio.muted = true;
  remoteAudio.play().catch(()=>{});

  /* mic */
  let localStream;
  try{
    localStream = await navigator.mediaDevices.getUserMedia({ audio:true });
  }catch{
    alert('Need microphone access'); joinBtn.disabled=false; return;
  }
  const micTrack = localStream.getAudioTracks()[0];

  await connectToOpenAI(micTrack);

  /* signalling WS */
  const proto = location.protocol==='https:'?'wss':'ws';
  ws = new WebSocket(`${proto}://${location.host}/ws`);
  ws.onopen = ()=>{
    ws.send(JSON.stringify({join:roomInput.value.trim()}));
    negotiateChat();
  };

  ws.onmessage = async ev=>{
    const msg = JSON.parse(typeof ev.data==='string'?ev.data:await ev.data.text());
    if(msg.offer){
      await pcChat.setRemoteDescription(msg.offer);
      const answer = await pcChat.createAnswer();
      await pcChat.setLocalDescription(answer);
      ws.send(JSON.stringify({answer}));
    }else if(msg.answer){
      await pcChat.setRemoteDescription(msg.answer);
    }else if(msg.ice){
      try{ await pcChat.addIceCandidate(msg.ice); }catch{}
    }
  };
}
</script>