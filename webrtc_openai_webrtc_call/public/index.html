<!doctype html>
<html>
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI-mediated WebRTC chat</title>


<style>
 body{font-family:sans-serif;text-align:center;margin-top:2em}
 input,button{font-size:1.05em}  button{margin-left:.3em}
 pre{max-width:90%;margin:1em auto;background:#f3f3f3;padding:.7em;
     text-align:left;white-space:pre-wrap}
</style>

<h2>GPT-4o talks only to the remote peer</h2>
<p>
  Room:
  <input id="roomInput" placeholder="choose-a-room">
  <button id="joinBtn" disabled>Join</button>
</p>

<audio id="remoteAudio" autoplay playsinline></audio>
<pre id="log"></pre>

<script>
/* ───────── logger ───────── */
const logEl = document.getElementById('log');
function log(...a){
  console.log('[DBG]', ...a);
  logEl.textContent += a.map(v=>typeof v==='string'?v:JSON.stringify(v)).join(' ') + '\n';
}

/* ───────── UI ───────── */
const roomInput   = document.getElementById('roomInput');
const joinBtn     = document.getElementById('joinBtn');
const remoteAudio = document.getElementById('remoteAudio');

roomInput.addEventListener('input', ()=>joinBtn.disabled = !roomInput.value.trim());
joinBtn.onclick = joinRoom;

/* ───────── globals ───────── */
let pcOai, pcChat, ws, relayCtx, statsTimer;
const keepAliveNodes = [];                 // prevents GC of Web-Audio nodes

/* helper: await ICE gathering */
const waitIce = pc => new Promise(res=>{
  if (pc.iceGatheringState === 'complete') return res();
  pc.addEventListener('icegatheringstatechange', function h(){
    if (pc.iceGatheringState === 'complete'){
      pc.removeEventListener('icegatheringstatechange', h); res();
    }
  });
});

/* helper: robust play() with retries + visibility awareness */
function ensurePlay(el, n = 0){
  el.play()
    .then(()=>log('audio playing'))
    .catch(err=>{
      if (document.visibilityState === 'hidden') {
        const h = ()=>{ document.removeEventListener('visibilitychange', h);
                        ensurePlay(el, n); };
        document.addEventListener('visibilitychange', h);
        return;
      }
      if (n < 5) setTimeout(()=>ensurePlay(el, n+1), 350);
      else log('⚠️  autoplay failed', err.name);
    });
}

/* ───────── OpenAI peer connection ───────── */
async function connectToOpenAI(micTrack){
  log('Fetching token …');
  const { client_secret } = await (await fetch('/session')).json();
  const TOKEN = client_secret.value;

  pcOai = new RTCPeerConnection({iceServers:[{urls:'stun:stun.l.google.com:19302'}]});
  pcOai.addTrack(micTrack);

  pcOai.createDataChannel('oai-events').onmessage = ev=>{
    try   { log('oai-event', JSON.parse(ev.data)); }
    catch { log('oai-raw',  ev.data); }
  };

  /*
   * Route model speech ONLY to the remote peer,
   * never to local speakers (avoids loop-back / echo).
   */
  pcOai.ontrack = ({streams})=>{
    const srcStream = streams[0];
    log('<< modelStream', srcStream.id);

    const srcNode = relayCtx.createMediaStreamSource(srcStream);
    const dstNode = relayCtx.createMediaStreamDestination();
    srcNode.connect(dstNode);
    keepAliveNodes.push(srcNode, dstNode);

    const aiTrack = dstNode.stream.getAudioTracks()[0];

    if (!pcChat){
      buildChatPC(aiTrack);
      ws?.readyState === 1 && negotiateChat();
    } else {
      const sender = pcChat.getSenders().find(s=>s.track?.kind==='audio');
      sender ? sender.replaceTrack(aiTrack) : pcChat.addTrack(aiTrack);
    }
  };

  await pcOai.setLocalDescription(await pcOai.createOffer());
  await waitIce(pcOai);

  const answerSDP = await fetch(
    'https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2025-06-03',
    {
      method : 'POST',
      headers: { Authorization:`Bearer ${TOKEN}`, 'Content-Type':'application/sdp' },
      body   : pcOai.localDescription.sdp
    }
  ).then(r=>r.text());

  await pcOai.setRemoteDescription({type:'answer', sdp:answerSDP});
  log('OpenAI SDP done');
}

/* ───────── browser ↔ browser connection ───────── */
function buildChatPC(firstTrack){
  pcChat = new RTCPeerConnection({iceServers:[{urls:'stun:stun.l.google.com:19302'}]});
  const sender = pcChat.addTrack(firstTrack);
  log('pcChat addTrack', firstTrack.id);

  /* Enable OPUS-DTX so silence is almost free */
  try{
    const p = sender.getParameters();
    (p.encodings||[]).forEach(e=>e.dtx='enabled'); sender.setParameters(p);
  }catch{}

  pcChat.onicecandidate = ({candidate})=>{
    candidate && ws?.readyState === 1 && ws.send(JSON.stringify({ice:candidate}));
  };

  pcChat.ontrack = ({track, streams})=>{
    log('>> ontrack', track.id);
    remoteAudio.srcObject = streams[0];
    remoteAudio.muted = false;
    remoteAudio.oncanplay = ()=>ensurePlay(remoteAudio);
  };

  clearInterval(statsTimer);
  statsTimer = setInterval(async ()=>{
    const stats = await pcChat.getStats();
    let inB = 0, outB = 0;
    stats.forEach(r=>{
      if (r.type==='inbound-rtp'  && r.kind==='audio') inB  += r.bytesReceived;
      if (r.type==='outbound-rtp' && r.kind==='audio') outB += r.bytesSent;
    });
    log('RTP bytes ▲', outB, ' ▼', inB);
  }, 3000);
}

async function negotiateChat(){
  if (!pcChat) return;
  await pcChat.setLocalDescription(await pcChat.createOffer());
  ws.send(JSON.stringify({offer: pcChat.localDescription}));
}

/* ───────── entry ───────── */
async function joinRoom(){
  joinBtn.disabled = true;
  log('joinRoom pressed');

  /* 1) mic permission */
  let localStream;
  try{
    localStream = await navigator.mediaDevices.getUserMedia({
      audio:{ echoCancellation:true, noiseSuppression:true }
    });
    log('mic permission granted');
  }catch{
    alert('Need microphone access');
    return joinBtn.disabled = false;
  }
  const micTrack = localStream.getAudioTracks()[0];

  /* 2) shared AudioContext */
  relayCtx = new (window.AudioContext||window.webkitAudioContext)();
  if (relayCtx.state === 'suspended') await relayCtx.resume();
  document.addEventListener('visibilitychange', ()=>{
    if (relayCtx.state === 'suspended') relayCtx.resume();
  });
  log('AudioContext state:', relayCtx.state);

  /* 3) prime <audio> element inside user-gesture */
  remoteAudio.muted     = true;
  remoteAudio.srcObject = new MediaStream();
  await remoteAudio.play().catch(()=>{});

  /* 4) OpenAI + signalling in parallel */
  connectToOpenAI(micTrack).catch(console.error);

  ws = new WebSocket(
    `${location.protocol === 'https:' ? 'wss' : 'ws'}://${location.host}/ws`
  );
  ws.onopen = ()=>{
    ws.send(JSON.stringify({join: roomInput.value.trim()}));
    negotiateChat();
  };
  ws.onmessage = async ev=>{
    const msg = JSON.parse(typeof ev.data === 'string' ? ev.data : await ev.data.text());
    if (msg.offer){
      await pcChat.setRemoteDescription(msg.offer);
      await pcChat.setLocalDescription(await pcChat.createAnswer());
      ws.send(JSON.stringify({answer: pcChat.localDescription}));
    } else if (msg.answer){
      await pcChat.setRemoteDescription(msg.answer);
    } else if (msg.ice){
      try { await pcChat.addIceCandidate(msg.ice); } catch{}
    }
  };
}
</script>
</html>