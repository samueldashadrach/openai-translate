<!doctype html>
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI-mediated WebRTC chat</title>

<style>
  body  { font-family:sans-serif; text-align:center; margin-top:2em; }
  input,button{ font-size:1.05em; }
  button{ margin-left:.3em; }
  #clickToUnmute{
    position:fixed; inset:0; background:#0009; color:#fff;
    display:flex; align-items:center; justify-content:center;
    font-size:1.4em; cursor:pointer; z-index:10;  /* hidden by default */
  }
  pre{ text-align:left; max-width:90%; margin:1em auto;
      background:#f3f3f3; padding:.7em; }
</style>

<h2>WebRTC chat – model’s voice only for the remote peer</h2>
<p>
  Room:
  <input id="roomInput" placeholder="choose-a-room">
  <button id="joinBtn" disabled>Join</button>
</p>

<audio id="remoteAudio" autoplay playsinline></audio>
<div id="clickToUnmute" hidden>Tap to unmute</div>
<pre id="log"></pre>

<script>
/* ---------- tiny logger ---------- */
const logEl = document.getElementById('log');
function log(...a){
  const txt = a.map(v=>typeof v==='string'?v:JSON.stringify(v)).join(' ');
  console.log('[DBG]', ...a);
  logEl.textContent += txt + '\n';
}

/* ---------- UI ---------- */
const roomInput   = document.getElementById('roomInput');
const joinBtn     = document.getElementById('joinBtn');
const remoteAudio = document.getElementById('remoteAudio');
const unmuteDiv   = document.getElementById('clickToUnmute');

roomInput.addEventListener('input', () =>
  joinBtn.disabled = roomInput.value.trim() === ''
);
joinBtn.addEventListener('click', joinRoom);
unmuteDiv.onclick = ()=>{ remoteAudio.muted=false; remoteAudio.play(); unmuteDiv.hidden=true; };

/* ---------- globals ---------- */
let pcOai, pcChat, ws;

/* helper: wait until pc.iceGatheringState === 'complete' */
function waitIceComplete(pc){
  return new Promise(res=>{
    if(pc.iceGatheringState==='complete') return res();
    pc.addEventListener('icegatheringstatechange', function h(){
      if(pc.iceGatheringState==='complete'){ pc.removeEventListener('icegatheringstatechange',h); res(); }
    });
  });
}

/* ---------- OpenAI connection ---------- */
async function connectToOpenAI(micTrack) {
  const { client_secret } = await (await fetch('/session')).json();
  const EPHEMERAL = client_secret.value;

  pcOai = new RTCPeerConnection({ iceServers:[{urls:'stun:stun.l.google.com:19302'}] });
  pcOai.addTrack(micTrack);

  pcOai.ontrack = ({ streams }) => {
    log('<< modelStream from OpenAI');
    const modelStream = streams[0];

    /* remote → local track via AudioContext */
    const ctx  = new (window.AudioContext||window.webkitAudioContext)();
    const src  = ctx.createMediaStreamSource(modelStream);
    const dest = ctx.createMediaStreamDestination();
    src.connect(dest);
    const sendTrack = dest.stream.getAudioTracks()[0];

    if (!pcChat) {
      buildChatPC(sendTrack);
      if (ws?.readyState === WebSocket.OPEN) negotiateChat();
    } else {
      const sender = pcChat.getSenders()
                           .find(s=>s.track && s.track.kind==='audio');
      if (sender) sender.replaceTrack(sendTrack); else pcChat.addTrack(sendTrack);
    }
  };

  /* signalling with OpenAI */
  await pcOai.setLocalDescription(await pcOai.createOffer());
  await waitIceComplete(pcOai);
  const answer = await fetch(
    'https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2025-06-03',
    { method:'POST',
      headers:{ Authorization:`Bearer ${EPHEMERAL}`, 'Content-Type':'application/sdp' },
      body:pcOai.localDescription.sdp }
  ).then(r=>r.text());
  await pcOai.setRemoteDescription({ type:'answer', sdp:answer });
}

/* ---------- browser ↔ browser ---------- */
function buildChatPC(firstTrack){
  pcChat = new RTCPeerConnection({ iceServers:[{urls:'stun:stun.l.google.com:19302'}] });
  pcChat.addTrack(firstTrack);

  pcChat.onicecandidate = ({candidate})=>{
    if (candidate && ws?.readyState===WebSocket.OPEN)
      ws.send(JSON.stringify({ice:candidate}));
  };

  pcChat.ontrack = ({ streams })=>{
    log('>> pcChat ontrack');
    remoteAudio.srcObject = streams[0];

    /* try to play; if rejected show overlay */
    remoteAudio.muted=false;
    remoteAudio.play().then(()=>log('audio playing'))
                      .catch(e =>{ log('play() blocked',e.name);
                                   unmuteDiv.hidden=false; });
  };
}

async function negotiateChat(){
  const offer = await pcChat.createOffer();
  await pcChat.setLocalDescription(offer);
  ws.send(JSON.stringify({offer}));
}

/* ---------- entry point ---------- */
async function joinRoom(){
  joinBtn.disabled = true;

  /* mic */
  let localStream;
  try{
    localStream = await navigator.mediaDevices.getUserMedia({ audio:true });
  }catch{
    alert('Need microphone access'); joinBtn.disabled=false; return;
  }
  const micTrack = localStream.getAudioTracks()[0];

  /* “prime” the audio element while we still have a user gesture */
  remoteAudio.muted = true;
  remoteAudio.play().catch(()=>{});        // ignore failure – just priming

  await connectToOpenAI(micTrack);

  /* signalling WS */
  const proto = location.protocol==='https:' ? 'wss':'ws';
  ws = new WebSocket(`${proto}://${location.host}/ws`);
  ws.onopen = ()=>{ ws.send(JSON.stringify({join:roomInput.value.trim()})); negotiateChat(); };

  ws.onmessage = async ev=>{
    const msg = JSON.parse(typeof ev.data==='string' ? ev.data : await ev.data.text());
    if(msg.offer){
      await pcChat.setRemoteDescription(msg.offer);
      const answer = await pcChat.createAnswer();
      await pcChat.setLocalDescription(answer);
      ws.send(JSON.stringify({answer}));
    }else if(msg.answer){
      await pcChat.setRemoteDescription(msg.answer);
    }else if(msg.ice){
      try{ await pcChat.addIceCandidate(msg.ice); }catch{}
    }
  };
}
</script>